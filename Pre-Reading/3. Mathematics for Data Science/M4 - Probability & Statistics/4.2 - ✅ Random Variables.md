---
Ex. Completed: false
tags:
  - maths
Links:
  - "[[4.3 - âœ… Expectations & Variance]]"
---
> [!note] Topic Overview
> (Definition)
> (Why it matters)
> 

# Definition
## Definition
A random variable (RV) is a function that assigns a number to each outcome in the sample space
- A way of turning outcomes (like words or labels) into numbers so we can calculate with them
- Notation - capital letters such as $X,Y,Z$
	- One specific outcome = $\omega \in \Omega$ 
	- $\omega \in \Omega$ is mapped to a number $X(\omega)$
### Example (coin flip)
- Sample space: $\Omega = \{H,T\}$
- Define random variable $X$:
	- $X(H) = 1$
	- $X(T)=0$ 
So the random variable turns the word outcomes ($H,T$) into numbers ($1,0$)
## Types of random variables
### Discrete random variables
- Take countable values (finite or countably infinite)
- Examples:
	- Die roll: $X \in \{1,2,3,4,5,6\}$
	- Number of website clicks per minute: $X \in \{0,1,2,...\}$
- The distribution of discrete random variables is described by a ***==probability mass function==***
	- The PMF tells you the probability of each of those values:
$$
P(X = x)
$$
Where:
- $P()$ -> probability operator
- $X$ -> the random variable -> represents outcome translated into a number
- $=$ -> means we are asking about the probability that $X$ takes a particular value
- $x$ -> A specific value that the random variable might take
### Continuous random variables
- Take values from an uncountably infinite set, usually real numbers or intervals
- Examples:
	- Height of a person: $X \in [100:250]$ cm
		- Random variable $X$ can take any real value between 100 and 250 cm
	- Response time in seconds: $X \in [0,\infty]$
- Distribution is described by ***==probability density function==*** (PDF):
	- We compute probabilities over intervals (definite integral - space under curve between 2 points)
## Data Science link
- *Discrete RVs* -> often model counts and categorical data (click-throughs, number of sales, class labels)
- *Continuous RVs* -> model measurements (height, income, processing time)
- Many ML algorithms assume a distribution for the random variables
	- Discrete -> binomial, Poisson
	- Continuous -> normal, exponential, uniform

# Probability Distributions (general)
## Probability mass function (PMF)
### Intro
Used for discrete random variables (countable infinity too)
- Notation:
$$
P(X = x)
$$
means: the probability that random variable $X$ takes the value $x$
### Properties
1. Probabilities are never negative
$$
P(X=x) \geq 0
$$
2. If you add up probabilities for all possible values of $X$, they must equal 1
$$
\sum P(X=x)=1
$$
## Probability density function (PDF)
### Intro
Used for continuous random variables - PDF is the function that describes the shape of the distribution over the whole sample space
- Notation:
$$
f(x)
$$
- Formula:
$$
P(a \leq X \leq b) = \int^b_a f(x)dx
$$
- $f(x)$ is the PDF
- $P(a \leq X \leq b)$ is the probability that $X$ falls in the interval $[a,b]$ (the area under the PDF between $a$ and $b$)

>[!note] Important Note
>$f(x)$ is not a probability. For continuous variables, the probability at an exact value is 0
## Cumulative distribution function
Works for both discrete and continuous random variables - for a random variable $X$, the CDF at a point $x$ is the probability the $X$ takes a value less than or equal to $x$
- Notation:
$$
F(x) = P(X \leq x)
$$
	- This means: probability that the random variable $X$ is $\leq x$
- How it relates:
	- For discrete RVs, the CDF is the sum of the PMF values up to $x$
	- For continuous RVs, the CDG is the integral of the PDF up to $x$:
$$
F(x) = \int_{\infty}^x f(t)dt
$$
### Examples
- Discrete - roll a die. $X$ = outcome
	- $F(3) = P(X \leq 3) = P(1) + P(2) + P(3) = \tfrac{1}{2}$
- Continuous - uniform(0,10)
	- $F(5) = P(X \leq 5) = \int_0^5 \tfrac{1}{10}dx =0.5$ 

# Common Distributions (examples)
## Discrete distributions (use PMFs)
### Bernoulli distribution
![[bernoulli_distribution.png]]
- Models a single trial where there are only two outcomes: success/failure
- Parameter:
	- $p = P(\text{success})$
- Success = 1, failure = 0
- PMF tells you probabilities of two values:
	- $P(X=1) = p$
	- $P(X=0)=1-p$
### Binomial distribution
![[binomial_distribution.png]]
- Extends Bernoulli - models the number of successes in $n$ independent Bernoulli trials
- Parameters: 
	- $n = \text{number of trials}$
	- $p = P(\text{success in one trial})$
Example: number of heads in 10 coin tosses:
- $n=10, \ p=0.5$
Probability of exactly 6 heads:
$$
P(X=6) = \begin{pmatrix} 10\\6 \end{pmatrix} (0.5)^6(0.5)^4 = \tfrac{10!}{6!4!}(0.5)^10 = 210 \cdot \tfrac{1}{1024} = 0.205 = 20.5\%
$$
### Geometric distribution
![[geometric_distribution.png]]
- Models the number of trials until the first success
- Parameter:
	- $p=P(\text{success per trial})$
Example: roll a die until you get a 6 (using )
- Success probability: $p=\tfrac{1}{6}$
- Probability first success is on the 3rd roll:
$$
P(X=3) = (1-\tfrac{1}{6})^2\cdot \tfrac{1}{6} \approx 0.116 \approx 11.6\%
$$
### Poisson distribution
![[poisson_distribution.png]]
- "What is the chance of exactly $k$ events happening in a fixed time/space interval, if on average $\lambda$ events occur"
- Parameter:
	- $\lambda$ -> number of events in a fixed interval of time
## Continuous distributions (use PDFs)
### Uniform distribution
![[uniform_distribution.png]]
- All outcomes in an interval are equally likely
	- We look at numbers in an interval $[a,b]$
	- Every number in the range has the same probability density
- Parameters:
	- $a$ -> lower bound
	- $b$ -> upper bound
### Normal (gaussian) distribution
![[normal_distribution.png]]
- Most important distribution in statistics (bell-shaped curve)
- Parameters:
	- $\mu$ -> mean (centre)
	- $\sigma$ -> SD (spread)
### Exponential distribution
![[exponential_distribution.png]]
- Models the waiting time until the next event
	- "if events happen at a constant average rate, how long until the next one"
- Exponential nature explained in example: buses per hour:
	- $\lambda = 1$ buses/hour -> average wait = $1/\lambda$ = 1 hour
	- $\lambda = 2$ buses/hour -> average wait = $1/2$ = 30 min
	- $\lambda = 3$ buses/hour -> average wait = $1/3$ = 20 min
	- The bigger $\lambda$ is, the faster the events happen on average
- Parameter:
	- $\lambda$ -> rate parameter
## Interpreting parameters
### Binomial
- $n$ -> number of trials
- $p$ -> probability of success
- Mean = Variance = $\lambda$
### Normal
- $\mu$ -> mean (centre)
- $\sigma^2$ -> variance (spread)
- About 68% of values lie within $\mu \pm \sigma$
### Poisson
- $\lambda$ -> expected number of events per interval
- Mean = Variance = $\lambda$
### Exponential
- $\tfrac{1}{\lambda}$ -> expected waiting time
# Transformations of Random Variables
## Linear transformations: $Y = aX + b$
### You can scale and shift a random variable
- $a$ = scale (stretch/compress)
- $b$ = shift (move up or down)
### Why it matters in DS
- Normalising features (e.g., rescaling income from dollars to thousands of dollars)
- Converting units (e.g., Celsius $\leftrightarrow$ Fahrenheit)
## Effect on mean and variance
### If $X$ has mean $\mu$ and variance $\sigma^2$
- Mean of $Y = aX +b$ is $E[Y] = a \mu +b$
	- $E[Y]$ -> mean of $Y$
	- It is simply transforming the mean $\mu$ with original transformation $Y=aX+b$
### Why it matters in DS
- Scaling features changes variance
- Shifting only moves the mean, doesn't affect variance or SD
## Standardisation: $Z=\tfrac{X_i-\mu}{\sigma}$
- Subtract the mean from data point $X_i$ and divide by SD
- Result: new variable $Z$ has mean 0 and variance 1
### Why it matters in DS
- Used everywhere in ML
- Lets you compare across variables with different units
# Joint Distributions
## Intro
A joint distribution describes the probability behaviour of two (or more) random variables at the same time
- $P(X=x, Y=y$
## Joint PMF/PDF
### Notation
For discrete random variables
$$
P(X=x, Y=y)
$$
- probability that $X$ takes value $x$ and $Y$ takes value $y$
For continuous random variables:
$$
f(x,y)
$$
- joint probability density function
### Example: roll a die twice:
$X$ = outcome of first roll, $Y$ = outcome of second roll
- JP = $P(X=3,Y=5)=\tfrac{1}{36}$
## Marginal distributions
To get the distribution of just one variable, sum (discrete) or integrate (continuous) over the other
- The probability of $X$ being the desired outcome(s) for each possible outcome of $Y$
### Example (discrete): from two dice rolls:
- Joint probability table = $6 \times 6$
- Marginal probability of $X=3$:
$$
P(X=3)=\sum_{y=1}^6P(X=3, Y=y)
=\tfrac{6}{36} = \tfrac{1}{6}
$$
- $\tfrac{6}{36}$ represents $\tfrac{1}{6}$ chance of $X=3$ for each of the outcomes of $Y$ (6 of them)
	- Thus, $\tfrac{1}{36} \cdot 6 = \tfrac{6}{36}$
## Independence
- Two variables $X$ and $Y$ are independent if:
$$
P(X,Y) = P(X) \cdot P(Y)
$$
- Meaning: knowing the value of one tells you nothing about the other
### Example: toss two separate fair coins
- Joint probability: $P(X=H, Y=H) = \tfrac{1}{4}$
- Individual probabilities multiplied: $\tfrac{1}{4} \times \tfrac{1}{4} = \tfrac{1}{2}$
# Applications in Data Science
- *Discrete RVs* -> modelling counts (clicks, arrivals, failures)
- *Continuous RVs* -> modelling measurements (weights, durations)
- *Normal distribution* -> basis of statistical inference, regression assumptions
- *Poisson* -> event counts in time/space
- *Joint distributions* -> feature dependencies in multivariate data
- *Standardisation* -> preprocessing before ML models