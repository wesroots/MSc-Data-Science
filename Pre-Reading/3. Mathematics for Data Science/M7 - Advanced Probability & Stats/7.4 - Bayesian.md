---
Ex. Completed: false
tags:
  - maths
Links:
  - "[[4.1 - ✅ Probability Basics]]"
---
> [!note] Topic Overview
> (Definition)
> (Why it matters)
> 

# Intro/Definition
## Definition
### Core Idea
Bayesian statistics is about updating what we believe about something uncertain as we see new data
### Conceptual idea
- Start with a prior belief about a parameter (e.g., what we think is true before collecting data)
- Collect new data, measure how likely that data would be if our belief were true
- Update belief using Bayes' theorem, producing a new posterior belief - a refined estimate based on both what we believed before and what we observed
### Importance
- Allows us to treat unknowns as probabilities, not fixed numbers
- Gives a flexible way to combine past knowledge with new evidence
### Distinction from hypothesis testing
Bayesian inference:
- Parameters are treated as random variables with probabilities
- Prior belief -> updates with data using Bayes' theorem
- Produces a posterior distribution -> probability of each hypothesis or parameter value
- Gives a probability-based belief update, not just yes/no
In short:
- Hypothesis testing -> "Is this data inconsistent with my assumption?"
- Bayesian inference -> "Given this data, how strongly should I believe each possible assumption?"

# Meaning in Probability and Statistics
## Prior Distribution
- Belief about parameter before data
- Example: belief that the effectiveness of a drug is 70%
- Expresses existing knowledge or assumptions
## Likelihood
- How probable observed data are for different possible parameter values
- Connects model to data - "how well does vector of parameter explain observed data"
- Example: if a drug was 70% effective in clinical setting, likelihood tells us how likely that result is for different assumed success rates
## Posterior Distribution
- Updated belief about parameter after seeing data
- Combines:
	- Prior belief
	- Likelihood from new data
- Follows Bayes' theorem:
$$
P(\theta \mid \text{data}) = \tfrac{P(\text{data}\mid \theta)P(\theta)}{P(\text{data})}
$$
Where:
- $P(\theta)$ -> prior
- $P(\text{data})$ -> normalising constant (overall $p$ of $\text{data}$ happening)
	- Called normalising constant as it makes the posterior a valid probability distribution
- $P(\theta \mid \text{data})$ -> posterior (updated belief)
- $P(\text{data}\mid \theta)$ -> likelihood of parameter ($p$ of observed data if parameter really were $\theta$)
## Intuitive Flow

## Example of Bayesian inference
### Problem: predicting whether a customer will churn (leave a subscription)
Model:
- $\theta$ = "customer will churn"
- Data $D$ = customer's recent activity dropped significantly
We need to find:
$$
P(\text{churn} \mid \text{drop in activity})
$$
**Step 1: Prior $P(\theta)$**
- $P(\theta)$ = $p$ all users churn
	- 10% of all users churn
**Step 2: Likelihood $P(\text{data} \mid \theta)$**
- Chance of seeing the data if the customer churns
	- 80% of customers who churn show drop in activity
	- $P(\text{drop}\mid \text{churn})=0.8$
**Step 3: Evidence $P(\text{data})$**
- $P(\text{drop in activity})$ = $p$ of seeing that behaviour in any customer
	- $P(\text{drop} = P(\text{drop} \mid \text{churn})\cdot P(\text{churn}) + P(\text{drop} \mid \text{churn}^c)\cdot P(\text{churn}^c)$ 
	- Say only 20% of non-churners drop activity:
		- $P(\text{drop}) = 0.8(0.1) + 0.2(0.9)=0.26$
**Step 4: Posterior $P(\theta \mid \text{data})$**
- Update belief using Bayes' theorem:
$$
P(\text{churn} \mid \text{drop}) = \frac{P(\text{drop} \mid \text{churn}) P(\text{churn})}{P(\text{drop})} = \frac{0.8 \times 0.1}{0.26} \approx 0.308
$$
$\therefore$ , after seeing the drop, belief that this customer will churn rises from 10% → 31%

# Application to Data Science
