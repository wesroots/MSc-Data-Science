---
Ex. Completed: false
tags:
  - maths
Links:
  - "[[2.1 - ✅ Differentiation]]"
  - "[[7.3 - ✅ Regression]]"
---
> [!note] Topic Overview
> (Definition)
> (Why it matters)
> 

# Intro to Optimisation
## Definition
Optimisation is the process of finding the best value of a function under given conditions
- Mathematically, this usually means finding the minimum or maximum of a function
### For a function $f(x)$
- Local minimum at $x=a$ -> $f(a)$ is smaller than values of $f(x)$ near $a$
- Global minimum: $f(a)$ is smaller than all values of $f(x)$
### We typically use derivatives:
- $f'(x) =0$ -> candidate for min/max (stationary point, gradient = 0)
- $f''(x)>0$ -> local min
- $f''(x) < 0$ -> local max
## Role in ML
In ML, optimisation is the engine behind model training:
- Models are defined by parameters $\theta$
- We define a loss function $L(\theta)$ to measure error between predictions and true values
- Training = finding $\theta^*$ that minimises $L(\theta)$
## Worked Example
### Simple function
$f(x) = x^2 + 2x +3$
- Derivative: $f'(x) = 2x +2$
- Solve $f'(x) = 0$ -> $x=-1$
- Second derivative: $f''(x) = 2>0$
	- $\therefore$ , minimum at $x=-1$, value $f(-1) = 2$

# Unconstrained Optimisation
## Definition
Optimisation where:
- Only one variable is involved ($f(x)$, not $f(x,y)$)
- No constraints (e.g., no requirement that $x>0$)
## Gradient/Derivative as a Condition for Optimum
- For a function $f(x)$, an optimum occurs where slope = 0
	- $f'(x) = 0$ -> stationary points
- To classify them:
	- $f''(x)>0$ -> local min
	- $f''(x) < 0$ -> local max
- Example in "Introduction to Optimisation"
### Newton-Raphson Method for Optimisation
Newton-Raphson is an iterative method to find where $f'(x) = 0$ (since optimum requires derivative = 0)
### Contrast with root finding
- Root finding: solve $f(x) = 0$ -> solving for equation solutions
- Optimisation: solve $f'(x)=0$ -> locating where function flattens

# Multivariable Optimisation
>[!note] Optimisation in more than 2D
## Gradient Vector ($\nabla f$)
$$
\nabla f(x) = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \dots, \frac{\partial f}{\partial x_n} \right)
$$
Where:
- $n$ -> number of input variables
- $\tfrac{\partial f}{\partial x_i}$ -> partial derivative of $x_i$
	- Partial derivatives exist in multivariable function differentiation
### Partial derivatives $\partial$ example
Take
$$
f(x,y) = x^2y +3y
$$
- $\tfrac{\partial f}{\partial x} = 2xy$
- $\tfrac{\partial f}{\partial y} = x^2+3$
In each partial derivative case, treat the other variables as constants
- $\therefore$ ,  the gradient vectors is a vector of both:
$$
\nabla f(x,y) = (2xy, x^2+3)
$$
### Interpretation of the gradient vector $\nabla f$
- Points in the direction of steepest ascent
- At an optimum: $\nabla f = 0$ 
## Hessian Matrix and Second-Order Conditions
The Hessian matrix is the matrix of the second derivatives
$$
H(f) =
\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots \\
\vdots & \vdots & \ddots
\end{bmatrix}
$$
### Example
Take
$$
f(x,y) = x^2 +y^2
$$
- Gradient: $\nabla f = (2x,2y)$. At $(0,0)$, gradient = $(0,0)$
- Hessian matrix:
$$
H = \begin{bmatrix} 2&0\\0&2 \end{bmatrix}
$$
### Interpretation
What it is
- $H$ is the square matrix of all second derivatives of a function, it captures the curvature of the surface in every direction
How to read it
- Diagonal entries: second derivatives wrt each variable separately
- Off diagonal: cross-derivatives (how variables interact - curvature in mixed directions)
How to interpret for optimisation
- In a diagonal Hessian, diagonals -> eigenvalues

# Gradient Descent Method
>[!note]
>This is the core optimisation algorithm in ML
## Iterative Update
Formula:
$$
x_{k+1} = x_k - \eta \nabla f(x_k)
$$
Where:
- $x_k$ -> current point (set of parameters)
- $\nabla f(x_k)$ -> gradient at that point (direction of steepest ascent)
- $\eta$ -> step size (learning rate)
- Update rule -> keep moving a little in the negative gradient direction until you reach a minimum
### Process:
1. Start with initial guess for $x_k$
2. Compute gradient $\nabla f(x_k)$
3. Update using formula
## Step Size
- If $\eta$ is too small -> slow convergence
- If $\eta$ is too large -> overshoot, possibly diverge
## Use in ML
- In ML, need to minimise loss function
	- For simple models, loss has a closed-form solution
	- For complex models, it doesn't
- Gradient descent provides a way to approximate the solution step by step
## Example (simple quadratic)
$$
f(x) = x^2
$$
- Gradient: $2x$
- Start at $x_0 = 4$, learning rate $\eta=0.1$
Updates:
- $x_1=3.2$
- $x_2=2.56$
- $x_3=2.048$
-> converges to 0 (the minimum)

# Application to Data Science
### Model training = optimisation problem
- Adjust model parameters to minimise a loss function
### Gradient descent and variants
- Core algorithms for training ML models
- Scale to high-dimensional parameter spaces
### Convex vs. non-convex problems
- Convex -> easy, one global minimum
- Non-convex -> harder, many local minima/saddle points
### Regularisation
- Adds penalties in optimisation to stop overfitting
### Clustering/dimensionality reduction
- Both are solved using optimisation
### Likelihood methods
- Fit probabilistic models by maximising likelihood
### Hyperparameter tuning
- Also an optimisation problem