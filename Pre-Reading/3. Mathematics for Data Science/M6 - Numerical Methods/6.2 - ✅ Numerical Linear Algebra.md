---
Ex. Completed: false
tags:
  - maths
Links:
  - "[[3.1 - ✅ Matrices]]"
---
> [!note] Topic Overview
> Computers have limited computing capacity and finite precision - using matrix factorisations and iterative methods (numerical methods) we can avoid inaccuracies in calculations
> - [The Bright Side of Mathematics - "LU Decomposition - An Example Calculation"](https://www.youtube.com/watch?v=BFYFkn-eOQk&t=329s) - Matrix factorisation method example
> - Conditioning
> 	- A property of the problem
> 	- How sensitive the true solution $x$ of $Ax=b$ is to tiny changes
> 	- Well conditioned: small in -> small out
> - Stability
> 	- A property of the algorithm used
> 	- How much the algorithm amplifies rounding errors
> 	- Stable algorithm: result close to true solution, even with floating-point error
> - Data Science
> 	- Efficiency of finding solution with enormous datasets
> 	- Makes solving for weights efficient and stable

# Intro
## What is Numerical Linear Algebra (NLA)
- **Linear algebra** -> study of vectors, matrices, and solving SLE
- **NLA** -> how we actually compute these things efficiently and stably on software
- **Numerical** -> practical algorithms computers use to get answers when formulas don't exist or are too messy
- Computers can't handle infinite precision, so algorithms matter: we care about speed, rounding errors, and stability
## SLE
### Methods of solving
- Direct -> solve exactly in a finite number of steps (Gaussian, LU decomposition)
- Iterative -> start with a guess and refine it - useful for very large systems
# Matrix Factorisations
## Intro/Why Factorisations Matter
- Solving $Ax=b$ directly is expensive and unstable for large systems
- Instead, we factorise $A$ into simpler matrices (triangular, orthogonal, diagonal, ...)
- Once factored, solving or optimising becomes much easier
## LU Decomposition
### Idea - factorise matrix $A$ into two simpler matrices
$$
A = LU
$$
- $L$ = lower triangular (entries below diagonal, with 1s on diagonal)
- $U$ = upper triangular (entries above diagonal)
- to solve $Ax=b$:
	- First solve $Ly = b$ by forward substitution (easy because $L$ is lower-triangular)
	- Then solve $Ux=y$ by back substitution (easy because $U$ is upper-triangular)
### Why useful
- If you need to solve the same system with many different $b$ values, you don't want to redo Gaussian elimination every time
- LU decomp. lets you reuse the factorisation of $A$-> saves a huge amount of computation
- In $Ax=b$, you're estimating components of $x$ based on given data $A,b$
	- LU decomp. allows computers to solve for $x$ based on fixed features $A$ and multiple outputs $b$
### Example  (from "The Bright Side of Mathematics" video)
$$
A=\begin{bmatrix} 2&4&3&5\\-4&-7&-5&-8\\6&8&2&9\\4&9&-2&14 \end{bmatrix}
$$
1. Factorise first column
$$
A=\begin{bmatrix} \color{purple}{2}&4&3&5\\\color{green}{-4}&-7&-5&-8\\\color{red}{6}&8&2&9\\\color{blue}{4}&9&-2&14 \end{bmatrix}
$$
$$
=\begin{bmatrix}1&&&\\\color{green}{-2}&1&&\\\color{red}{3}&&1&\\\color{blue}{2}&&&1\end{bmatrix} \begin{bmatrix} 2&4&3&5\\0&1&1&2\\0&-4&-7&-6\\0&1&-8&4 \end{bmatrix}
$$
2. Repeat for remaining columns
$$
= 
\underbrace{
\begin{bmatrix}
1 & 0 & 0 & 0 \\
-2 & 1 & 0 & 0 \\
3 & -4 & 1 & 0 \\
2 & 1 & 3 & 1
\end{bmatrix}
}_{L}
\cdot
\underbrace{
\begin{bmatrix}
2 & 4 & 3 & 5 \\
0 & 1 & 1 & 2 \\
0 & 0 & -3 & 2 \\
0 & 0 & 0 & -4
\end{bmatrix}
}_{U}
= A
$$
## QR Decomposition
### Factorisation:
$$
A = QR
$$
where:
- $Q$ -> orthogonal matrix ($Q^{T}Q = I$), meaning its columns are perpendicular and well-scaled
	- Built from $A$ -> 
- $R$ -> upper triangular matrix
### Why useful:
- Orthogonal matrices are numerically stable to work with
	- Numerically stable: when a computer works with the matrix (solving, inverting, etc.) small rounding errors don't blow up into big errors in the result
- QR is widely use in least squares regression
## Cholesky Decomposition
### Factorisation:
$$
A = LL^T
$$
where $L$ is lower triangular
- Works only if $A$ is a symmetric positive definite
	- Symmetric: $A=A^T$
	- Positive definite: all eigenvalues > 0
### Why useful:
- Very efficient and numerically stable compared to LU
- Used in optimisation algorithms
- Used in statistics for covariance matrices

# Iterative Methods
## Intro/What Iterative Methods Are
- Direct methods (Gaussian, LU, QR) give an exact solution in a fixed number of steps
- Iterative methods instead start with a guess for $x$ in $Ax=b$, then refine it step by step until it converges to the solution
>[!note] Iterative methods are different from matrix factorisations
## Jacobi Method
1. Take each equation in the system and isolate one variable in terms of the others
2. Start with guesses for $x_1, x_2,...$
3. All updates for a step are done simultaneously
4. Conceptually simple, but slow
## Gauss-Seidel Method
- Updates each unknown immediately and uses the updated values in the same iteration
- Usually converges faster than Jacobi
## Convergence
- Not every iterative method always converges
- Whether it works depends on properties of $A$
- Key idea - check if repeated updates bring the guess closer to the true solution
## Why Useful
- For very large, sparse systems
	- Direct methods (like LU are too expensive)
	- Iterative methods only need local updates - much more efficient
- In ML and simulations, you often don't need the exact solution, just within tolerance
	- Iterative methods let you stop once the error is within tolerance

# Conditioning & Stability
## Definitions
- **Condition** -> property of the problem itself
	- Tells you how sensitive the true solution $x$ of $Ax=b$ is to tiny changes in $A$ or $b$
	- Problem sensitive?
- **Stability** -> property of the algorithm used
	- How much algorithm amplifies rounding errors
	- Method sensitive?
## Condition Number
- The condition number of matrix $A$ measures how sensitive the solution $x$ is to small changes in the input $b$ or in $A$
- Notation: $\kappa(A)$ ($\kappa$ = `\kappa`)
### Interpretation
- If $\kappa(A)$ is small (close to 1), small input changes -> small output changes
- If $\kappa(A)$ is large, tiny input changes -> big changes in the solution
## Ill-Conditioned vs. Well-Conditioned Matrices
### Well-conditioned
- ↓ condition number number
- Solution is stable and reliable
### Ill-conditioned
- ↑ condition number
- Solution may swing wildly if you change inputs a tiny bit
## Round-Off Errors and Numerical Stability
- Computers store numbers with finite precision
- Tiny rounding errors can accumulate in calculations
- If matrix is ill-conditioned, these errors get magnified
- Numerical stability means: the algorithm controls error growth and gives a reliable approximation
# Applications in Data Science
### Solving normal equations in regression
- Regression parameters come from solving $(A^TX)\beta = x^Ty$, which is just $Ax=b$
- Factorisations make this efficient and stable
### Matrix decomp. for PCA
- PCA reduces data dimensions by finding directions of maximum variance
- Requires eigenvalues/eigenvectors or singular value decomposition, built on matrix factorisations
### Efficient algorithms for high-dimensional data
- Datasets in ML are huge (many features, many samples)
- Direct solving is too costly; efficient NLA algorithms handle these large systems without exploding in time or memory