---
Ex. Completed: true
Confidence:
tags:
  - maths
Links:
  - "[[3.1 - ✅ Matrices]]"
  - "[[3.5 - ✅ Variance & Covariance]]"
---
> [!note] Topic Overview
> - [3Blue1Brown Video](https://www.youtube.com/watch?v=PFDu9oVAE-g&ab_channel=3Blue1Brown)
> - Eigenvectors are (nonzero) vectors associated with (and only with) a **square matrix** that when multiplied by associated eigenvalues maintain direction (but not magnitude)
> ![[Screenshot 2025-09-03 at 14.54.40.png]]
> Green and yellow vectors are eigenvectors of this matrix transformation with eigenvalues 3 and 2 respectively
> - Generally, matrices have eigenvectors and eigenvalues. When you try and solve for $\lambda$, this happens:
> $$
> \begin{array}
> \det(\begin{bmatrix} -\lambda & -1 \\ 1 & -\lambda \end{bmatrix}) = (-\lambda)(-\lambda) (-1)(1) \\
> \\
> = \lambda^2 + 1 = 0 \\ \\
> \therefore \lambda = i \ \text{or} \ \lambda = -i
> \end{array}
> $$
> - Data Science - eigenvectors describe main directions and strengths in data
> 	- PCA - reduces dimensionality by keeping directions with the largest eigenvalues (most variance

# Intro
## Key equation
$$
Mv = \lambda v
$$
Where:
- $M$ = transformation matrix
- $v$ = a nonzero vector
- $\lambda$ = the eigenvalue(s)

Demonstrates that the matrix transformation applies the same transformation as the eigenvalue
- Can be rewritten as $$(M-\lambda)v = 0$$
which helps with the characteristic equation
## The characteristic equation
$$
\det(M-\lambda) = 0
$$
As $v \neq 0$ -> $det(M-\lambda)$ must equal zero for the characteristic equation to be zero
- Knowing that $det(M-\lambda) = 0$ allows us to solve for $\lambda$
## Solving procedure
Example: find eigenvalues and eigenvectors of the matrix:
$$
A = \begin{bmatrix} 4&2\\1&3 \end{bmatrix}
$$
### Step 1 - Use the characteristic equation to solve for $\lambda$
$$
\begin{array}{c}
\det(\begin{bmatrix} 4-\lambda&2\\1& 3-\lambda \end{bmatrix}) = 0 \\
\\
(4-\lambda)(3-\lambda) - (-2)(1) = 0 \\
\\
\therefore \lambda = 5, 2
\end{array}
$$
### Step 2 - Plug $\lambda$ values back into key equation
$$
\begin {array} {c}
(M-\lambda)v =0
\\
\\
(\begin{bmatrix} 4-5&2\\1& 3-5 \end{bmatrix})v = 0
\end {array}
$$
### Step 3 - Solve for $v_1$ and $v_2$
$$
\begin{array} {c}
\begin{bmatrix} -1&2\\1& -2 \end{bmatrix} \begin{bmatrix} x\\y \end{bmatrix}= 0
\\
\\
-x +2y = 0 \rightarrow x = 2y\\
x -2y = 0 \rightarrow x = 2y
\\
\\
\therefore \text{when}\ \lambda = 5, \\ v_1 = 
\begin{bmatrix} 2\\1 \end{bmatrix}
\end{array}
$$
Repeat for $\lambda = 2$ to get $v_2 = \begin{bmatrix} 1\\-1 \end{bmatrix}$

# Properties
## Eigenvalues
- An $n \times n$ matrix has **up to** $n$ eigenvalues (counting multiplicity)
- All distinct eigenvalues -> always exactly $n$ eigenvectors
- Repeated eigenvalues -> may give $>n$ unique eigenvalues, but symmetric matrices still give $n$ eigenvectors
	- Defective matrices - repeated -> $\leq n$ eigenvectors (exist in theory, not needed for MSc applications)
## Eigenvectors
- Every eigenvalue has at least 1 eigenvector
- If eigenvalues are all distinct, you always get $n$ independent eigenvectors
![[Screenshot 2025-09-07 at 13.37.06.png]]
# Diagonalisation
## The idea and formula
- Some square matrices (all symmetrical - some non symmetrical aren't) can be simplified into diagonal form in order to multiply
	- This simplified form is:
$$
A = PDP^{-1}
$$
Where:
- $D$ -> diagonal matrix (all of-diagonal entries = 0) 
	- It's diagonal entries are the eigenvalues of $A$
- $P$ -> is the matrix whose columns are the eigenvectors of $A$
- $P^{-1}$ -> is the inverse of $P$
Note:
- Matrix multiplication is not cumulative, so $P$ and $P^{-1}$ cannot move past $D$ to cancel
- Order of eigenvalues in $D$ and eigenvectors in $P$ and $P^{-1}$ do not matter as long as you are consistent

## Why it matters
- Computing powers ($A^n$) is messy
	- If $A = PDP$, then
$$
A^n = (PDP^{-1})^n = PD^nP^{-1}
$$
$D^n$ is easy as raising a diagonal matrix to a power means raising each diagonal entry to that power

>[!note] Important note
>A matrix must have $n$ linearly independent eigenvectors to be diagonalised (symmetric matrices always satisfy this condition)
# Symmetric Matrices
## Definition
A matrix $A$ is symmetric if
$$
P = P^T
$$
i.e., entries are mirrored across the diagonal. E.g.:
$$
\begin{bmatrix} 4&2\\2&3 \end{bmatrix} \quad \text{is symmetric}
$$
## Key properties
- All real eigenvalues
	- No complex ones
- Orthogonal eigenvectors
- Always diagonalisable
	- Any symmetric matrix has $n$ independent eigenvectors
	- $A = PDP^T$
		- Note: $P^{-1}$ becomes $P^T$ as $P$ is orthogonal ($A^{-1} = A^T$)

# Data Science Link
## Covariance matrix
- Covariance matrix used to measure how variables vary together
	- It is symmetric -> it can be diagonalised
- Diagonalising tells you:
	- Eigenvectors -> the directions in which the data varies the most (PCs)
	- Eigenvalues -> how strong that variation is (variation size)
