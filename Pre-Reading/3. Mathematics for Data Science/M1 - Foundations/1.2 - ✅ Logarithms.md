---
Ex. Completed: true
tags:
  - maths
Links:
  - "[[2.1 - ✅ Differentiation]]"
  - "[[7.3 - ✅ Regression]]"
---
> [!note] Topic Overview
> ## Definition
> $$
> \log_b(y) = x \;\Longleftrightarrow\; b^x = y, \quad b > 0, \; b \neq 1, \; y > 0$
> $$
> - $\log_b(y)$ means "the power you raise $b$ by in order to get $y$"
> ## Use
> - Generally: undo exponential growth/decay, simplifies complexity into manageable values
> - In data science:
> 	- Feature scaling - log transforms compress extremes and make data more normal like -> useful for regression and ML
> 	- Prob. & stats. - log-likelihood functions easier to differentiate ([[4.3 - ✅ Expectations & Variance]])
> 
> #maths

# Logarithms
## Laws of logarithms
- Product -> $\log_a(xy) = log_ax + log_ay$
- Quotient -> $log_a(\tfrac{x}{y}) = log_ax-log_ay$
- Power -> $log_a(x^n) = nlog_ax$
- Change of base -> $log_ab = \tfrac{log_cb}{log_ca}$
## Special Logs
- $ln(x)$ = natural log = $log_ex$
- $log_{10}(x)$ = common log
## Solving equations with logs
- Use logs to solve exponentials: $3^x = 20 \;\Longrightarrow\; x=log_3(20)$
- Combining with algebra to isolate unknowns
## Graphs of log functions
- Shape of $y = log_b(x)$
- Relationship with exponential $y = b^x$
	- ![[IMG_9BF2F3C92A4D-1 1.jpeg]]
## Data Science application
### Log transformations: reduces skew in data
![[output (1).png]]
- Not trying to preserve picture of "how many poor vs rich" there are
- Logs turn multiplicative changes (£10k -> £20k feels very different from £100k -> £110k)
	- $log(ab) = log(a)+log(b)$ 
- Growth by x2 then x3 becomes just + $log(2)$ and + $log(3)$
![[Pasted image 20250902220836.png]]
### Log-likelihood in stats and ML
- Likelihood: probability of data under model; log-likelihood = log of that
- Why logs: products into sums, underflow (where likelihood of many events collapses to zero), makes calculus easier
### Log-loss
- Log loss = a way to measure how good or bad a prediction is in classification (logistic regression, neural nets)
	- It penalises wrong predictions more if they're made with high confidence
$$
\text{Log-loss} = -\Big[ y \log(\hat{p}) + (1-y)\log(1-\hat{p}) \Big]
$$
The closer $\hat{p}$ is to 1, the smaller the loss
