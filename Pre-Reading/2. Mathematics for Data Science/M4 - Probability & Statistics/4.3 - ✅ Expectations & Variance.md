---
Ex. Completed: true
tags:
  - maths
Links:
  - "[[3.5 - ✅ Variance & Covariance]]"
  - "[[7.2 - ✅ Estimation & Testing (Statistical Inference)]]"
  - "[[7.3 - ✅ Regression]]"
---
> [!note] Topic Overview
> (Definition)
> (Why it matters)
> 
# Expectation (mean)
## What expectation means
- The expectation is the average outcome you'd expect, i.e., the mean
- Probability weighted-average of all possible values a random variable can take
- Notation for the expected value (mean) of random variable $X$:
$$
E[X]
$$
## Discrete random variables
If $X$ can take values $x_1,x_2,x_3,...$ with probabilities $P(X=x_i)$ then:
$$
E[X] = \sum_xxP(X=x)
$$
Meaning:
- Multiply each possible value by its probability ($xP(X=x)$)
- Add them all up ($\sum_x$)
### Example: fair die
$$
E[X] = 1\cdot \tfrac{1}{6} + 2\cdot \tfrac{1}{6}+...+ 6\cdot \tfrac{1}{6} = 3.5
$$

## Continuous random variables
If $X$ is continuous with PDF $f(x)$:
$$
E[X] = \int_{-\infty}^{\infty} xf(x) dx
$$
This is the continuous version of the same idea - instead of summing, you integrate
### Example: uniform (0,1)
$$
f(x) = 1 \quad (0 \leq x \leq 1)
$$
$$
E[X]=\int_0^1x\cdot 1dx = [\tfrac{x^2}{2}]^1_0 = 0.5
$$
So the mean of a uniform (0,1) variable is 0.5
## Linearity of expectation
Scaling a random variable scales its mean
$$
E[aX+bY]=aE[X]+bE[Y]
$$
Where:
- $a$ and $b$ -> constants to scale
- $X$ and $Y$ -> random variables
- $E[\cdot]$ -> expected mean
### Example
- Let $X$ = die roll ($E[X]=3.5$)
- Let $Y$ = coin flip ($E[Y] = 0.5$)
Then:
$$
E[2X+3Y] = 2E[X]+3E[Y]=8.5
$$
### Useful because:
- Makes expectations easy to compute
- Loss functions - expected error of a model often breaks into sums
- Feature engineering - expected values of weighted sums of features
- Ensemble methods - expected prediction of an ensemble
- Stochastic processes - expected counts, waiting times, rewards
# Variance
## Definition
Variance measures the spread of a random variable around its mean
$$
\text{Var}(X_i) = E[(X_i-E[X])^2]
$$
Where:
- $E[X]$ -> mean of $X$
- $X_i-E[X]$ -> deviation of $X_i$ from mean
### Equivalent form
$$
\text{Var}(X) = E[X^2] - (E[X])^2
$$
This is simple algebra - a computational shortcut
- Often easier to calculate $E[X^2]$ directly than to compute deviations
## Properties
### Scaling and shifting
$$
\text{Var}(aX+b) = a^2\text{Var}(X)
$$
- Multiply data by $a$: spread scales by $a^2$
- Add constant $b$: variance doesn't change (shifting doesn't affect spread)
### Variance of sums
$$
\text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y) \quad \text{if independent}
$$
- Variance add when variables are independent

# Standard Deviation
## Definition (formula)
$$
\sigma_X = \sqrt{\text{Var}(X)}
$$
- Variance is average squared distance from the mean -> SD rescales it back into the same units as the data
## Interpretation
- Typical distance of a data point from the mean
- Variance gave us squared distances (less intuitive)
# Covariance and Correlation
## Covariance
Formula:
$$
\text{Cov}(X,Y) = E[(X-E[Y])(Y-E[Y])]
$$
- Measures how two variables vary together
- Interpretation (tends to be in terms of frequency):
	- Positive -> when $X$ is above its mean, $Y$ also tends to be above its mean
	- Negative -> when $X$ is above its mean, $Y$ tends to be below its mean
	- Near zero -> no consistent relationship
## Independence and covariance
- If $X$ and $Y$ are independent -> covariance = 0
- Reverse isn't always true:
	- $\text{Cov}$ can be 0 even if $X$ and $Y$ are dependent in more complicated ways (nonlinear)
## Correlation
Formula:
$$
\rho(X,Y) = \tfrac{\text{Cov}(X,Y)}{\sigma_X\sigma_Y}
$$
Where:
- $\rho$ -> representation for correlation value
- $\sigma_X$ -> SD of $X$ ($\sqrt{\text{Var}(X)}$)
### Correlation
- Correlation = scaled version of covariance
- Normalises covariance -> always lies between -1 and +1
### Interpretation
- $\rho = +1$: perfect positive linear relationship
- $\rho = -1$: perfect negative linear relationship
- $\rho = 0$: no linear relationship
# Application in Data Science
- *Expectation* -> average predicted outcome (loss functions, risk)
 - *Variance* -> uncertainty/noise in data
 - *Covariance/correlation* -> dependence between features (feature selection, PCA)
 - *Standardisation* -> subtract mean, divide by STD - used in preprocessing for ML