---
Ex. Completed: true
tags:
  - maths
Links:
  - "[[4.2 - ✅ Random Variables]]"
---
> [!note] Topic Overview
> (Definition)
> (Why it matters)

# Foundations
## Experiment, outcome, sample space $\Omega$
### Experiment
Any process or action with uncertain results. E.g., heads in a coin toss, rolling a 4 on a die
### Outcome
A single possible result of the experiment
### Sample space ($\Omega$)
The set of all possible outcomes.
- E.g.:
	- Coin toss: $\Omega = \{H,T\}$
	- Die roll: $\Omega = \{1,2,3,4,5,6\}$
## Events as subsets of $\Omega$
An event is any subset of the sample space
- E.g., rolling an even number on a die: $A=\{2,4,6\} \subset \Omega$
- An event can be:
	- A single outcome -> "elementary event"
	- A set of outcomes
	- The entire sample space ($\Omega$) -> "certain event"
	- The empty set ($\varnothing$) -> "impossible event"
## Axioms of probability (Kolmogorov)
Kolmogorov defined probability with three axioms (axiom: rule accepted without proof - in P&S, axioms are starting rules that define how probability works)
### 1. Non-negativity
For any event $A$,
$$
P(A)\geq0
$$
### 2. Normalisation
Probability of the whole sample space is 1,
$$
P(\Omega) =1
$$
### 3. Additivity
For disjoint (mutually exclusive) events $A$ and $B$,
$$
P(A\cup B) = P(A) + P(B)
$$
These axioms are the foundation of modern probability theory
## Probability of an event (equally likely outcomes)
When all outcomes in $\Omega$ are equally likely:
$$
P(A) = \tfrac{|A|}{|\Omega|}
$$
where:
- $|A|$ = number of favourable outcomes
- $|\Omega|$ = total number of outcomes
Examples:
- Rolling a 6 on a fair die:
	- $|A| =1, \ |\Omega| = 6 \Rightarrow P(A) = \tfrac{1}{6}$
- Rolling an even number
	- $|A| = 3, \ |\Omega| =6 \Rightarrow P(A) = \tfrac{3}{6}=\tfrac{1}{2}$

# Rules of Probability (+independence)
## Complement rule
The complement rule says that the probability of an event not happening is just 1 minus the probability of it happening:
$$
P(A^c) = 1-P(A)
$$
- $P(A^c)$ -> the probability of the complement of $A$ (not $A$)
## Addition rule
Finding the probability that at least one of two events happens:
$$
P(A\cup B) = P(A) + P(B) - P(A\cap B)
$$
- $P(A\cap B)$ is subtracted to avoid double-counting outcomes where both occur
## Conditional probability
Finding the probability of one event given another has already happened:
$$
P(A \ | \ B) = \tfrac{P(A\cap B)}{P(B)}
$$
- Meaning:
	- The probability of $A$ given that $B$ has occurred
- Example: from a deck of 52 cards:
	- $A$: draw a king
	- $B$: draw a face card (Jack, Queen, King)
	- $P(A\cap B) = \tfrac{4}{52}, \ P(B) = \tfrac{12}{52}$
$$
P(A \ | \ B) = \tfrac{4/52}{12/52} = \tfrac{1}{3}
$$
## Multiplication rule
Finding the probability of two events happening together:
- If dependent (conditional:
$$
P(A\cap B) = P(A \ | \ B) \cdot P(B)
$$
- If independent:
$$
P(A\cap B) = P(A) \cdot P(B)
$$
## Independence
Two events $A$ and $B$ are independent if knowing that one occurs does not change the probability of the other (above formula)
### Disjoint (mutually exclusive)
- Events cannot happen at the same time
	- $P(A\cap B) = 0$
- Example: rolling a die: $A = \{2\}, \ B=\{5\}$. They cannot occur together
### Important contrast
If two events are disjoint (other than trivial case where probability of one even is zero), they cannot be independent
- They are dependent because one event occurring prevents the other from occurring
	- E.g., if you roll a 5 on a die, 2 cannot happen as 5 has already been rolled

# Bayes' Theorem
## Statement
Bayes' theorem gives a way to "flip" conditional probabilities
- Normally you might know $P(B \ | \ A)$: the probability that $B$ happens after $A$ occurs
- In practice, what you often want is $P(A \ | \ B)$: the probability that the cause of $A$ is true given that $B$ was observed
$$
P(A \ | \ B) = \tfrac{P(B \ | \ A)\cdot P(A)}{P(B)}
$$
Where:
- $P(A)$ -> prior probability of $A$ (belief before seeing $B$)
- $P(B \ | \ A)$ -> likelihood of observing $B$ if $A$ is true
- $P(B)$ -> evidence (overall probability of $B$, across all possibilities)
- $P(A \ | \ B)$ -> posterior probability of $A$ after observing $B$
It updates beliefs about $A$ after new evidence $B$
>[!note] Important Note
>You have to know the probability of $P(B \mid A^c)$ in order to find $P(A \mid B)$
## Example (medical testing)
### Suppose:
- Disease prevalence -> $P(A) = 0.01$ (1% of people have it)
- Test sensitivity -> $P(B \ | \ A) =0.95$ (detects 95% of cases)
- False positive rate -> $P(B \ | \ A^c) = 0.05$ (5% of the time)
### Then:
$$
P(B) = P(B \mid A)\cdot P(A) + P(B \mid A^c)\cdot P(A^c)
$$
$$
 = 0.95 \cdot 0.01 + 0.05 \cdot 0.99 = 0.059
$$
### Posterior:
$$
P(A \mid B) = \tfrac{0.95 \cdot 0.01}{0.059} \approx 0.161
$$

## Applications
- Classification -> in ML, estimate the probability of a data point belonging to a class given its features
- Spam filtering -> compute $P(\text{spam} \mid \text{words in email})$
	- Using frequencies of words in spam vs. non-spam emails
- Medical testing -> given a positive test result, compute $P(\text{disease} \mid \text{positive test})$

# Data Science Relevance
### Complement/addition rules
Event probabilities in exploratory data analysis
### Conditional probability
Basis for inference, supervised learning
### Independence
Assumption in many models
### Bayes' theorem
Foundation of Bayesian methods, probabilistic classifiers
# Exercise Set 1
>[!note] Question 1 (Theory)  
>Define “experiment,” “outcome,” and “sample space” in probability, and give one original example not already in your notes.  

- Experiment -> any action that has an uncertain outcome(s)
- Outcome -> a single possible result of an experiment
- Sample space -> the set of all possible outcomes
- Example: a roulette spin
	- Experiment -> spinning the wheel
	- Outcome -> landing on single slot, e.g., 17 red
	- Sample space -> the set of all slots it can possibly land in (either 37 or 38 slots)

---

>[!note] Question 2 (Theory)  
>What does it mean to say that an event is a subset of the sample space? Give two examples: one elementary event and one compound event.  

- Event is a subset of sample space -> includes one or more possible outcomes from the set of the whole sample space
- Elementary -> a die roll landing on 4 *{4}*
- Compound -> a die roll being even *{2,4,5}*

---

>[!note] Question 3 (Theory)  
>State Kolmogorov’s three axioms of probability in your own words, and explain why they are needed as a foundation.  

1. Probability of sample space is 1
2. *For disjoint events, $A$ and $B$, $P(A\cup B) = P(A) + P(B)$*
3. *Probability can never be negative (always $\geq 1$)*

---

>[!note] Question 4 (Practice)  
>A die is rolled. Using the equally likely outcomes rule, compute the probability of rolling (a) a number greater than 4, and (b) an odd number.  

![[Pasted image 20250923125939.png]]

---

>[!note] Question 5 (Theory)  
>Explain the complement rule in words. Why is it sometimes easier to calculate the probability of the complement instead of the event itself?  

- Complement of $P(A)$: $P(A^c)$
- Sometimes the complement may be easier to calculate if there are less outcomes or simpler probabilities

---

>[!note] Question 6 (Practice)  
>Two cards are drawn from a deck without replacement. Use the addition rule to compute the probability that at least one card is an Ace.  

![[Pasted image 20250923134523.png]]

---

>[!note] Question 7 (Theory)  
>In your own words, explain what conditional probability means. Why is it important in data science?  

- Conditional probability -> the probability that an event occurs given another one occurs first
- Importance -> *underpins prediction - many tasks involve updating probabilities when new data is observed*

---

>[!note] Question 8 (Practice)  
>From a standard deck: event A = drawing a King, event B = drawing a face card. Work out $(P(A \mid B))$.  

![[Pasted image 20250923135938.png]]

---

>[!note] Question 9 (Theory)  
>Explain the multiplication rule in your own words. What is the key difference in the formula when events are independent vs. dependent?  

- Finding the probability of two events both occurring
- $P(A \cap B) = P(A) \cdot P(B)$ if independent
- *$P(A \cap B) = P(A|B) \cdot P(B)$ if dependent*

---

>[!note] Question 10 (Theory)  
>Give an example of two independent events and two disjoint events. Explain clearly why independence and disjointness are not the same.  

- Independent:
	- Two dice being rolled
	- A card being drawn, put back, then drawn again
- Disjoint:
	- *Outcomes when rolling a die - 2 and 5 cannot be rolled at the same time*
	- *Drawing a random card from a deck - a face card and a non face card cannot be drawn at the same time*
- Difference:
	- *Events cannot be independent and disjoint at the same time - if event $A$ happens, it means $B$ cannot happen, it changes the probability of $B$ occurring to $0$*

---

>[!note] Question 11 (Theory)  
>Why can two disjoint events (other than trivial cases) never be independent?  

- Events cannot be independent and disjoint at the same time - if event $A$ happens, it means $B$ cannot happen, it changes the probability of $B$ occurring to $0$
- Trivial cases (both):
	- *Disjoint because it never overlaps with anything - if $P(A) = 0$, $A$ will never happen regardless of outcome*
	- *Independent because if $P(A)=0$, it cannot happen at all, and thus cannot affect another event's probability. Another event also cannot affect $P(A)$ as it was already zero*

---

>[!note] Question 12 (Theory)  
>Explain Bayes’ theorem in words: what does it let us do that ordinary conditional probability does not?  

- Bayes' theorem:
- *Lets us reverse conditional probability. Allows us to compute $P(A\mid B)$ using $P(B\mid A)$ together with prior probabilities of $A$ and $B$*
- *This makes it possible to update the probability of a cause after observing an effect*
---

>[!note] Question 13 (Practice)  
>For the medical testing example in your notes:  
>- Disease prevalence $(P(A) = 0.01)$, test sensitivity $(P(B \mid A)=0.95)$, false positive rate $(P(B \mid A^c)=0.05)$.  
>- Recalculate $(P(A \mid B))$ and interpret what it means in plain English.  

![[Pasted image 20250926104146.png]]

---

>[!note] Question 14 (Theory)  
>In Bayes’ theorem, what roles do the terms “prior,” “likelihood,” “evidence,” and “posterior” play?  

- *Prior ($P(A)$) -> belief about event $A$ before seeing evidence*
- *Likelihood ($P(B \mid A$)-> probability of $B$ given $A$ is true*
- *Evidence ($P(B)$) -> overall probability of observing "evidence"*
- *Posterior ($P(A \mid B)$) -> updated belief about $A$ after taking evidence into account*

---

>[!note] Question 15 (Theory)  
>Explain how Bayes’ theorem applies in spam filtering. What are the “events” A and B in this context? 

- $A$ -> a spam call
- *$B$ -> the call contains certain features (e.g., unknown number, odd hours, specific words etc.)*
- *Bayes' theorem:*
	- *Prior -> $P(A)$: how often spam calls are in general*
	- *$P(B)$: the probability calls contain certain features in general*
	- *Likelihood -> $P(B \mid A)$: probability spam calls show a feature*
	- *Posterior -> $P(A \mid B)$: updated probability it is a spam call given the features*

---

>[!note] Question 16 (Theory)  
>In your own words, explain why conditional probability is a foundation for supervised learning.  
>- (Supervised learning: type of ML where the algorithm is trained on labeled data (input–output pairs))

*Supervised learning is about learning how outcomes depend on inputs. Every classifier or regression model is, in one way or another, trying to approximate $P(Y∣X)\cdot P(Y \mid X)\cdot P(Y∣X)$.*

---

>[!note] Question 17 (Theory)  
>Why is independence such an important assumption in models like Naive Bayes? What would happen if the independence assumption fails badly?  

---

>[!note] Question 18 (Theory)  
>What does the “condition” of using the equally likely outcomes rule require? Give an example where outcomes are not equally likely.  

---

>[!note] Question 19 (Practice)  
>Suppose you toss a biased coin where \(P(H)=0.7\). Compute the probability of (a) getting at least one head in 2 tosses, and (b) getting two tails in a row.  

![[Pasted image 20250926123156.png]]

---

>[!note] Question 20 (Theory)  
>Summarise the data science relevance of probability basics: how do complement/addition rules, conditional probability, independence, and Bayes’ theorem appear in practice?  
