---
Ex. Completed: true
tags:
  - maths
Links:
  - "[[3.2 - ✅ Systems of Equations]]"
  - "[[3.3 - ✅ Vector Spaces]]"
  - "[[3.4 - ✅ Eigenvalues & Eigenvectors]]"
  - "[[3.5 - ✅ Variance & Covariance]]"
---
>[!warning] Important Note:
>- In practice, almost all matrices encountered in MSc Data Science are symmetric
>	- Symmetric matrix: a square matrix that is equal to its transpose - $a_{ij} = a_{ji}$
>		- E.g.: $\begin{bmatrix} 2&5\\5&3 \end{bmatrix}$ - the off diagonal entries (5) match

> [!note] Topic Overview
> - Matrices are tables of numbers arranged in rows and columns
> 	- Represent:
> 		- Multiple equations at once
> 		- Data in compact form (Python, Pandas)
> 		- Transformations
> 	- Relate to [[3.3 - ✅ Vector Spaces]] - a vector is just a matrix with one column
> - Data Science
> 	- Data representation
> 	- Linear regression - model $y = X\beta$
> 		- $y$ - output vector
> 		- $X$ - matrix of input data
> 		- $\beta$ - parameter vector
> - Definitions:
> 	- Dot product
> 		- Multiplying two vectors of the same length
> 		- Multiply matching entries and add
> 		- Outputs single scalar
> 	- Outer product
> 		- Two vectors
> 		- Multiply one as column and other as row
> 		- Outputs a matrix
> 	- Matrix multiplication
> 		- Two matrices $A_{m \times n}$ and $B_{n \times p}$
> 		- Each entry is the dot product of a row of $A$ with a column of $B$
> 		- Outputs a matrix of size $m \times p$

# Vectors basics
## Intro
A vector is an ordered list of numbers, a matrix with one column:
$$\vec{v} = \begin{bmatrix} 2 \\ 5 \\ 7 \end{bmatrix}$$
- In data science, each row can be seen as a vector of features:
	- E.g., height, age, weight
## Magnitude
How long the vector is in space
$$
||\vec{v}|| = \sqrt{v_1^2 + v_2^2 + . . .,+ v_n^2}
$$
## Scalar multiplication
Multiply a vector by a single number ("scalar") -> stretches, shrinks or flips
## Matrix multiplication (row/column rule)
Outer product gives a number, inner product gives a matrix
Take two vectors of the same length. Multiply corresponding entries, then add them up
$$C = \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ 5 & 6 \end{bmatrix} \quad (3 \times 2), \quad D = \begin{bmatrix} 7 & 8 \\ 9 & 10 \end{bmatrix} \quad (2 \times 2)$$
### Steps:
1. Multiply $R_1$ of $C$ with $C_1$ of $D$:
	- $1 \times 7 + 2 \times 9 = 25$ 
2. Multiply $R_1$ of $C$ with $C_2$ of $D$:
	- $1 \times 8 + 2 \times 10 = 28$
3. Multiply $R_2$ of $C$ with $C_1$ of $D$:
	- $3 \times 7 + 4 \times 9 = 57$
4. Repeat for all rows
Answer: $CD = \begin{bmatrix} 25 & 28 \\ 57 & 64 \\ 89 & 100 \end{bmatrix}$
### Formal equation:
$$
A_{m \times n} \times B_{n \times p}
$$
Therefore $A$'s number of rows and $B$' number of columns have to be the same
## Dot product (for row and column vectors)
The dot product rule for column and row vectors is not the same as the dot product for box matrices.
### Steps
Each item is multiplied by the corresponding item in the other vector
- E.g.:
$$
A = \begin{bmatrix} 1&2&3 \end{bmatrix} \quad B = \begin{bmatrix} 4&5&6 \end{bmatrix}
$$
Dot product = $\quad 1\times4 \quad + \quad 2\times5 \quad+\quad 3\times6$ 
- $=32$
# Types of Matrices
## Row, column, square
- Row = only one row
- Column = only one column
- Square = same number of rows and columns
## Zero and identity matrices
- Zero matrix = all entries are zero
- Identity matrix = like 1 for matrices
$$\begin{bmatrix}1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$$
## Diagonal matrices
Square with numbers only on diagonal and zero elsewhere:
$$\begin{bmatrix}2 & 0 & 0 \\ 0 & 5 & 0 \\ 0 & 0 & 7 \end{bmatrix}
$$

# Matrix Operations
## Addition and subtraction
Only works if matrices have the same dimensions
$$
\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} 
+ 
\begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix} 
= 
\begin{bmatrix} 6 & 8 \\ 10 & 12 \end{bmatrix}
$$
## Scalar multiplication
Multiply every entry by the same constant
$$
2 \cdot \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} 
= \begin{bmatrix} 2 & 4 \\ 6 & 8 \end{bmatrix}
$$
## Matrix multiplication
Rule: if $A$ is $m \times n$ and $B$ is $n \times p$, the $AB$ exists and is $m \times p$
$$
A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}, \quad 
B = \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix}, \quad 
AB = \begin{bmatrix} 19 & 22 \\ 43 & 50 \end{bmatrix}
$$
>[!warning] Important
>$AB$ is not the same as $BA$
>
>$AB$ means "take rows of $A$ and multiply with columns of $B$"
>$BA$ means "take rows of $B$ and multiply with columns of $A$"

# Special Matrices in Multiplication
## Identity matrix
Rule: multiplying by $I$ leaves a vector or matrix unchanged
- $AI = IA = A$
$$
\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} \times \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} =
\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}
$$
Why it matters:
 - $I$ acts like 1 for matrices
 - Used in regression solutions
 - Essential in optimisation and linear algebra
## Zero matrix
All entities are zero
- Rule: multiplying any vector/matrix by 0 gives 0
$$
0 = \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix}
$$
Why it matters:
- Zero wipes out information
- Useful for testing properties and in sparse-matrix representations
## Transpose $A^T$
Operation: flip rows into columns
$$
A = \begin{bmatrix} 1&2&3 \\ 4&5&6 \end{bmatrix}, \quad A^T = \begin{bmatrix} 1&4 \\ 2&5 \\ 3&6 \end{bmatrix}
$$
### Why it matters:
- Transpose is needed for dot products and inner products
- Appears in regression and ML
	- Normal equation: $\beta = (X^TX)^{-1}X^Ty$
- In stats: covariance matrix uses transpose $(X-\mu^T)(X-\mu)$
## Normal equation $\beta = (X^TX)^{-1}X^Ty$ (making predictions)
The normal equation is the formula that gives the best weights $\hat{\beta}$ for linear regression, without trial and error
### 1. Predicting an outcome
Formula:
$$
\hat{y} =\beta_0 + \beta_1x
$$
where $\beta_0$ = intercept, $\beta_1$ = slope
### 2. Write in matrix form
$$
X = \begin{bmatrix}
1 & 2 \\
1 & 3
\end{bmatrix}, \quad
y = \begin{bmatrix}
50 \\
65
\end{bmatrix}, \quad
\beta = \begin{bmatrix}
\beta_0 \\
\beta_1
\end{bmatrix}
$$
Model $\hat{y} = X\beta$
### 3. Define the error
The error vector is how far off each prediction (based on parameters) is off from observed outcomes - in minimised squares regression each prediction falls exactly on the regression line.
$$
e = y -X\beta
$$
As a perfect prediction $y$ will equal $X\beta$, the difference between them is the error vector $e$
### 4. State the goal
We want the smallest $e$ given the correct values of $\beta$
- i.e., the minimised sum of squared errors:
$$
\min_\beta||e||^2 = \min_\beta||y-X\beta||^2
$$
- $\min_\beta||e||^2$ -> the min value of $e$ using $\beta$ parameters
### 5. Using the transpose
The squared norm is defined as:
$$
||v||^2 = v^Tv
$$
so:
$$
||y-X\beta||^2 = (y-X\beta)^T(y-X\beta)
$$
### 7. Expand and differentiate $\tfrac{d}{d\beta}$, set to 0
$$
X^TXB = X^Ty
$$
### 8. Solve for weights
If $X^TX$ is invertible:
$$
\hat{\beta} = (X^TX)^{-1}X^Ty
$$

# Determinant
## Definition
For a 2x2 matrix
$$
A = \begin{bmatrix} a&b \\ c&d \end{bmatrix}
$$
the determinant is defined as:
$$
det(A) = ad-bc
$$
For matrices larger than 2x2 determinants are defined recursively
## Geometric meaning
- The determinant tells you how a matrix scales space when applied as a transformation
- For a 2x2 -> scaling factor for area
- For a 3x3 -> scaling factor for volume
- E.g.,:
	- If $det(A) = 2$ the transformation doubles areas/volumes
	- If $det(A) = -1$ it preserves size but flips orientation
## Determinant = 0
If $det(A) = 0$, the transformation squashes space into a lower dimension - e.g., a 2D square mapped into a flat line
- This means the matrix has no inverse

Data Science relevance:
- In statistics, determinants are crucial in multivariate distributions
- A determinant of 0 means your data/features are linearly dependent (no unique solution)

# Inverse of a Matrix
## Definition
- Inverse of matrix $A$ is written $A^{-1}$
	- It satisfies:
$$
A^{-1}A = AA^{-1} = I
$$
Like $a^{-1} = 1/a$ for numbers, except with matrices
## Existence condition 
A matrix only has an inverse if:
$$
det(A) \neq 0
$$
If $det(A) = 0$, the matrix is "singular" -> no inverse exists
## Formula for the 2x2 case
For
$$
A = \begin{bmatrix} a&b \\ c&d \end{bmatrix}
$$
the inverse is:
$$
A = \tfrac{1}{ad-bc} \times \begin{bmatrix} d&-b \\ -c&a \end{bmatrix}
$$
## Importance in solving equations $Ax = b$
Linear systems often written as $Ax =b$
- If $A$ is invertible ($det(A) \neq 0$),:
$$
x = A^{-1}b
$$
