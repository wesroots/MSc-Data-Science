---
Ex. Completed: true
tags:
  - maths
Links:
  - "[[3.4 - ✅ Eigenvalues & Eigenvectors]]"
---
> [!note] Topic Overview
> - A vector is an ordered list of numbers, formally an element of $\mathbb{R}^n$  (n-dimensional real space)
> 	- Geometric - represents direction and magnitude in space
> 	- Algebraic - a data container (like an array)
> 	- Statistical/DS - a feature vector (set of attributes about one data point)
> - Relation to [[3.1 - ✅ Matrices]] - a matrix is a collection of vectors, often written as a stacked column of vectors (side by side)
> $$
> A =\begin{bmatrix} 1&2&3 \\ 4&5&6 \\ 7&8&9 \end{bmatrix}
> $$
> Solution matrix would be written as:
> $$
> \begin{bmatrix} x \\ y \\ z \end{bmatrix}
> $$
> This explains why dot product is rows $\times$ columns - coefficient 1 ($1$) is multiplied with $x_1$ ($x$), coefficient 2 ($2$) with $x_2$ ($y$) and so on
> - Relation to [[3.2 - ✅ Systems of Equations]] - systems of linear equations can be written as
> $$
> Ax = b
> $$
> SLE show how matrices and vectors interact
> - Data Science:
> 	- Datasets are stored as matrices of vectors (rows = data points, columns = features)
> 	- Models use linear combinations of vectors
> 	- Simplifying data (PCA) means finding the most important directions in the space


# Introduction to Vectors
## Definition
Vectors are an ordered list of numbers
$$
\mathbf{v} = (2, -1, 3)
$$
Each number represents something about the vector (coordinates, feature values, etc.)
### n-dimensions notation:
$$
\mathbf{v} = (v_1, v_2, ..., v_n) \in\mathbb{R}^n
$$
## Interpretation
- Geometry: describes a position (a point in space) or direction (arrow from origin)
- Data science: represents a *data point* with features
	- Example: (height, weight, age) is just a 3D vector

# Vector Operations
## Addition and subtraction
Add or subtract corresponding components
$$
\begin{bmatrix} 2\\3\\1 \end{bmatrix} + \begin{bmatrix} -1\\4\\0 \end{bmatrix} = \begin{bmatrix} 1\\7\\1 \end{bmatrix}
$$
## Scalar multiplication (stretching/shrinking)
- Multiply each component of the vector by a scalar
$$
3 \times \begin{bmatrix} 2\\3\\1 \end{bmatrix} = \begin{bmatrix} 6\\-3\\12 \end{bmatrix}
$$
## Linear combination (building new vectors from others)
Combine vectors by multiplying them with scalars and adding
$$
2 \begin{bmatrix} 1\\0 \end{bmatrix} + (-3)\begin{bmatrix} 0\\1 \end{bmatrix} = \begin{bmatrix} 2\\-3 \end{bmatrix}
$$
### Real life example (CV disease prediction example):
In data science regression models are linear combinations of features:
- Features vector values = $\mathbf{x}$
	- Adipose tissue, BMI, height
- Weights vector = $\mathbf{w}$
	- Values in $\mathbf{w}$ are used as scalars that tell model how much each feature contributes to predicting $y$ in:
		- $y = w_1 \times \text{adipose tissue} \quad + \quad w_2 \times \text{BMI} \quad + \quad w_3 \times \text{sex}$
	- Weights are what are adjusted to find least regression in SOE
- In least squares regression, the model automatically adjusts the weights so that predictions are as close as possible to actual outcomes across all data points. More significant features in $\mathbf{x}$ have larger weights
## Using multiple samples to solve for weights vector
Prediction exam score $y$ from 4 features: hours studied, attendance, sleep, caffeine
### Step 1: Data
$$
X = \begin{bmatrix} 2&1&7&0\\5&1&6&1\\3&0&8&0 \end{bmatrix}, \quad y=\begin{bmatrix} 65\\80\\70 \end{bmatrix}
$$
- $X$ is $3 \times 4$ (3 students, 4 features)
- $y$ is $3 \times 1$ true exam scores
### Step 2: Weight vector (unknowns to learn)
$$
w = \begin{bmatrix} w_1\\w_2\\w_3\\w_4 \end{bmatrix}
$$
### Step 3: Predictions
$$
\hat{y} = Xw = \begin{bmatrix} 2&1&7&0\\5&1&6&1\\3&0&8&0 \end{bmatrix}\begin{bmatrix} w_1\\w_2\\w_3\\w_4 \end{bmatrix}
$$
Expanded to:
$$
\hat{y} = \begin{bmatrix} 2w_1&1w_2&7w_3&0w_4\\5w_1&1w_2&6w_3&1w_4\\3w_1&0w_2&8w_3&0w_4 \end{bmatrix}
$$
### Step 4: Learning weights
In practice: software computes minimised square error $||Xw -y||^2$
### Step 5: After training
Suppose solving gives:
$$
w = \begin{bmatrix} 5\\3\\7\\-2 \end{bmatrix}
$$
Now for any new student with features $x = (4,1,6,0)$, prediction is
$$
\hat{y} = w^{\top}x = 5(4) + 3(1) + 7(6) + -2(0) = 65
$$

# Magnitude and Normalisation
## Magnitude
The magnitude of a vector $\mathbf{v} = (v_1, v_2,..., v_n)$
$$
||\mathbf{v}|| \sqrt{v_1^2 + v_2^2 + ... + v_n^2}
$$
Example:
$$
\mathbf{v} = (3,4) \rightarrow ||\mathbf{v}|| = \sqrt{3^2 + 4^2} = 5
$$
Geometrically: distance of vector from origin
## Unit vectors
A unit vector is a vector of length 1 - made by dividing a vector by its magnitude:
$$
\hat{\mathbf{v}} = \tfrac{\mathbf{v}}{||\mathbf{v}||}
$$
Purpose: keeps only direction - removes scale (as magnitude becomes 1)
## Why normalisation matters (Data Science)
- In ML, features can have different scales
- If left unscaled, the feature with the largest numbers can dominate distance-based models
- Normalisation makes features comparable
There are different types of normalisation:
- Min-max
- Others (L2 norm, Z-score)

# Dot Product (Inner Product)
## Formula
Two vectors with the same length:
$$
\mathbf{a} = (a_1, a_2, ..., a_n), \quad \mathbf{b} = (b_1, b_2, ..., b_n)
$$
then the dot product is:
$$
\mathbf{ab} = a_1b_1 + a_2b_2 + ... + a_nb_n
$$
## Geometric Meaning (angle between vectors link)
Dot product is also:
$$
\mathbf{ab} = ||\mathbf{a}|| ||\mathbf{b}|| cos\theta
$$
where $\theta$ is the angle between vectors
### Interpretation:
- If vectors point in same direction -> dot product is large and positive
- If vectors are perpendicular -> dot product = 0
- If vectors point in opposite directions -> dot product is negative

# Angle Between Vectors
## Formula
Derived from the dot product generic form, rearranging gives the formula for angle:
$$
\cos{\theta} = \tfrac{\mathbf{ab}}{||\mathbf{a}|| ||\mathbf{b}||}
$$
Interpretations:
- $\theta = 0º$ -> exactly the same direction
- $\theta = 90º$ -> perpendicular (no relation)
- $\theta = 180º$ -> opposites

## Cosine similarity
As
$$
\begin{array}{c}
\cos(180) = -1 \\
\text{and} \\
\cos(0) = 1
\end{array}
$$
the cosine similarity tells you how similar the direction of two vectors are ranging from -1 (opposite), to 0 (unrelated) to +1 (identical direction)
## Data Science use
- Clustering - group documents, users, items that point in similar directions in feature space
- Recommendation - systems measure similarity between user preferences and item features
- Example - Netflix might represent a user's watched genres as a vector, and a movie's genre mix as a vector. Cosine similarity tells how well they align

# Comparison Methods (Summary)
## Magnitude
- Compares the lengths of vectors
- Ignores direction
## Dot product
- Formula: multiply matching components and sum
- Depends on magnitude and direction
- Big values = large directions
- Ignores distinction between "long but slightly different" vs "short but perfectly aligned"
## Cosine similarity
- Dot product divided by magnitudes
- Focuses only on angle between vectors
- Ranges: -1 -> 0 -> 1
- Ignores lengths