---
Ex. Completed: true
tags:
  - maths
Links:
---
> [!note] Topic Overview
> ![Normalized Nerd - Covariance Clearly Explained!](https://www.youtube.com/watch?v=TPcAnExkWwQ&ab_channel=NormalizedNerd)
> - Variance measures how spread out a single variable is, covariance measures how two variables vary together
> - Data Science
> 	- Covariance matrix - organises variances and covariances of all variables into one symmetric matrix
> 	- PCA - diagonalises the covariance matrix to find directions of maximum variance
> 	- Feature relationships - reveals redundancy or dependence between features

# Variance & Covariance
## Variance
Variance = how scattered/concentrated data points are around the mean -> squared distances from mean
### Variance formula:
$$
\text{Var}(x) = \tfrac{1}{n-1} \sum_{i=1}^n (x_i-\bar{x})^2
$$
Where:
- $(x_i - \bar{x})^2$ -> the square distance of point $x_i$ from the mean ($\bar{x}$)
## Covariance
Covariance = how two variables vary together
### Covariance formula:
$$
\text{Var}(x) = \tfrac{1}{n-1} \sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})
$$
![[Screenshot 2025-09-06 at 10.12.41.png]]
# Covariance Matrix
## Definition
The covariance matrix is a square matrix that stores:
- Variances of each variable (on the diagonal)
- Covariances between pairs of variables (off-diagonal)
### Covariance matrix:
For $p$ variables $X_1, X_2, ..., X_p$:
$$
\Sigma =
\begin{bmatrix}
\text{Var}(X_1) & \text{Cov}(X_1, X_2) & \cdots & \text{Cov}(X_1, X_p) \\
\text{Cov}(X_2, X_1) & \text{Var}(X_2) & \cdots & \text{Cov}(X_2, X_p) \\
\vdots & \vdots & \ddots & \vdots \\
\text{Cov}(X_p, X_1) & \text{Cov}(X_p, X_2) & \cdots & \text{Var}(X_p)
\end{bmatrix}
$$
### The components
- Positive - move together
- Negative - move oppositely
- Zero - unrelated
Components ($\text{Var}$ and $\text{Cov}$)
## Key Properties
### Symmetric
$\text{Cov}(X_i, X_j)= \text{Cov}(X_j, X_i)$
### Diagonal entries = variances
Big diagonals -> variables with large spread
### Covariance matrices are always positive semidefinite
## Positive semidefinite (PSD)
A matrix is positive semidefinite (PSD) if, for any vector $z$:
$$
z^{\top} Az > 0
$$
This is the quadratic form
- Positive -> never negative
- Semidefinite -> it can be zero
### How can entries be negative
Example:
$$
\Sigma = \begin{bmatrix} 4&-2\\-2&3 \end{bmatrix}
$$
- $-2$ off diagonals are covariances
- If tested with $z^TAz$ -> always $\geq 0$
